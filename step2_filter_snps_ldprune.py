__author__ = 'Mary T. Yohannes'

# This script performs SNP filtering, LD pruning and subset dataset accordingly using PLINK - input = QC'ed NeuroGAP PLINK files

import hailtop.batch as hb
import hail as hl

# get file size to allocate resources for batch jobs accordingly
def get_file_size(file):
    file_info = hl.utils.hadoop_stat(file)
    size_bytes = file_info['size_bytes']
    size_gigs = size_bytes / (1024 * 1024 * 1024)
    return size_gigs

# Step 1: perform snp and sample filters
# only generate the filtered snp list and fam file to avoid generating .bed file
def filter_snps(b, plink_files, storage_size):
    j = b.new_job(name='filter snps and samples')  # define job and label it
    j.image('hailgenetics/genetics:0.2.37')  # Docker image with PLINK2
    j.storage(f'{storage_size}Gi')  # increase storage according to file size
    j.cpu(16)
    j.memory('highmem')

    j.declare_resource_group(ofile={
        'snplist': '{root}.snplist',
        'fam': '{root}.fam',
        'rmdup.list': '{root}.rmdup.list',
        'log': '{root}.log'
    })

    # NGP5434-1XJK isn't present in the metadata and doesn't have a duplicate so it will be excluded from subsequent analysis
    j.command('echo NGP5434-1XJK NGP5434-1XJK > remove_sample.txt')

    j.command(f'''
    plink2 --bfile {plink_files} \
    --maf 0.05 \
    --geno 0.01 \
    --mind 0.01 \
    --rm-dup force-first list \
    --remove remove_sample.txt \
    --write-snplist \
    --make-just-fam \
    --out {j.ofile} ''')

    j.command(f'echo {j.ofile.snplist}') # for batch to pick up on what j.ofile.snplist is so that we don't run into an error when writing it out by itself below

    j.command(f"echo {j.ofile['rmdup.list']}") # in order to write it out by itself

    return j

# run LD pruning - 180k-400k

# Step 2: prune to remove highly correlated SNPs
# perform pruning with a window size of 200 variants, sliding across the genome with step size of 100 variants at a time, and filter out any SNPs with LD r^2 > 0.2 - based on RICOPILI suggestions
# keep only the first instance of each duplicate-ID variant
def ld_prune(b, bg_files, ofile1, storage_size):
    j = b.new_job(name='LD prune')  # define job and label it #f'{label}-clumping'
    j.image('hailgenetics/genetics:0.2.37')  # Docker image with PLINK2
    j.storage(f'{storage_size}Gi')  # increase storage according to file size
    j.cpu(16) # set cpu
    j.memory('highmem')

    j.declare_resource_group(ofile={
        'prune.in': '{root}.prune.in',
        'rmdup.list': '{root}.rmdup.list',
        'log': '{root}.log'
    })

    j.command(f'''
    plink2 --bfile {bg_files} \
    --keep {ofile1.fam} \
    --extract {ofile1.snplist} \
    --rm-dup force-first list \
    --indep-pairwise 200 100 0.2 \
    --out {j.ofile} ''')

    j.command(f"echo {j.ofile['rmdup.list']}") # in order to write it out by itself

    return j

# Step 3: generate a new set of PLINK files with filtered variants and samples
def generate_files(b, plink_files, ofile1_fam, pruned_snps, storage_size):
    j = b.new_job(name='generate a filtered set of PLINK files')  # define job and label it
    j.image('hailgenetics/genetics:0.2.37')  # Docker image that contains PLINK
    j.storage(f'{storage_size}Gi')  # increase storage according to file size
    j.cpu(16)
    j.memory('highmem')

    j.declare_resource_group(ofile={
        'bed': '{root}.bed',
        'bim': '{root}.bim',
        'fam': '{root}.fam',
        'log': '{root}.log'})

    j.command(f''' 
    plink2 --bfile {plink_files} \
    --keep {ofile1_fam} \
    --extract {pruned_snps} \
    --make-bed \
    --out {j.ofile} ''')

    return j

if __name__ == '__main__':

    backend = hb.ServiceBackend(billing_project='neale-pumas-bge', remote_tmpdir='gs://neurogap-bge-imputed-regional/mary/khat_gwas/tmp')  # set up backend

    b = hb.Batch(backend=backend, name='step2: filter SNPs, perform LD pruning, and subset PLINK files')  # define batch

    # QC'ed NeuroGAP dataset - generated by Lerato
    plink_files = b.read_input_group(**{
        'bed': 'gs://neurogap-bge-imputed-regional/lerato/wave2/plink_files/all_sites_all_phenos.bed',
        'bim': 'gs://neurogap-bge-imputed-regional/lerato/wave2/plink_files/all_sites_all_phenos.bim',
        'fam': 'gs://neurogap-bge-imputed-regional/lerato/wave2/plink_files/all_sites_all_phenos.fam'})

    # get file size
    plink_size = get_file_size('gs://neurogap-bge-imputed-regional/lerato/wave2/plink_files/all_sites_all_phenos.bed')
    storage_size1 = round(plink_size + 10)

    # run Step 1
    run_filter_snps = filter_snps(b, plink_files, storage_size1)

    # write out the snp list after filtering variants
    b.write_output(run_filter_snps.ofile['snplist'], f'gs://neurogap-bge-imputed-regional/mary/khat_gwas/intermediate_files/NeuroGAP_filtered_run1.snplist')

    # write out the duplicated-ID variants
    b.write_output(run_filter_snps.ofile['rmdup.list'], f'gs://neurogap-bge-imputed-regional/mary/khat_gwas/intermediate_files/NeuroGAP_filtered_ldpruned_run1.rmdup.list')

    # run Step 2
    run_ld_prune = ld_prune(b, plink_files, run_filter_snps.ofile, storage_size1)

    # write out the duplicated-ID variants
    b.write_output(run_ld_prune.ofile['rmdup.list'], f'gs://neurogap-bge-imputed-regional/mary/khat_gwas/intermediate_files/NeuroGAP_filtered_ldpruned_run1_dup2.rmdup.list')

    # get file size
    storage_size2 = round(plink_size + (plink_size/2) + 5)

    # run Step 3
    run_generate_files = generate_files(b, plink_files, run_filter_snps.ofile['fam'], run_ld_prune.ofile['prune.in'], storage_size2)

    # write out filtered and pruned PLINK files
    b.write_output(run_generate_files.ofile, f'gs://neurogap-bge-imputed-regional/mary/khat_gwas/intermediate_files/NeuroGAP_filtered_ldpruned_run1')

    b.run(open=True, wait=False)  # run batch

    backend.close()










